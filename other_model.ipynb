{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83cc91df-b6d2-4702-b269-9d26c39844c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os, gc\n",
    "import torch, torchvision\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import datetime\n",
    "\n",
    "from other_models.Other_Models import *\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b3a65a-4e3e-468c-bb2a-d6305b134727",
   "metadata": {},
   "source": [
    "### 计算距离函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "490ced50-33e9-487d-a0b8-7ff235fe83e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_km(lat1, lon1, lat2, lon2):\n",
    "    lat1, lon1, lat2, lon2 = lat1.astype('float'), lon1.astype('float'), lat2.astype('float'), lon2.astype('float')\n",
    "    # 批量计算地球上两点间的球面距离\n",
    "    R = 6371e3  # 地球半径（米）\n",
    "    phi_1, phi_2 = np.radians(lat1), np.radians(lat2)\n",
    "    delta_phi = np.radians(lat2 - lat1)\n",
    "    delta_lambda = np.radians(lon2 - lon1)\n",
    "    a = np.power(np.sin(delta_phi / 2), 2) + np.cos(phi_1) * np.cos(phi_2) * np.power(np.sin(delta_lambda / 2), 2)\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    return R * c / 1000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26b73ff-9c19-44e8-a7cf-e697516ac887",
   "metadata": {},
   "source": [
    "### 数据集相关类和函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caf56c1d-70dc-4c3b-8d39-fbea963348e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_scalar(x, scaler=None, inverse=False):\n",
    "    if inverse and (scaler is not None):\n",
    "        shape = x.shape\n",
    "        x = x.reshape(shape[0], -1)\n",
    "        x = scaler.inverse_transform(x)\n",
    "        x = x.reshape(shape)\n",
    "    else:\n",
    "        shape = x.shape\n",
    "        scaler = MinMaxScaler()\n",
    "        x = x.reshape(shape[0], -1)\n",
    "        x = scaler.fit_transform(x)\n",
    "        x = x.reshape(shape)\n",
    "    return x, scaler\n",
    "\n",
    "\n",
    "class RNN_TrainLoader(Dataset):\n",
    "    def __init__(self, x, y, standard):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.s = standard\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.x[item], self.y[item], self.s[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "\n",
    "class CNN_TrainLoader(Dataset):\n",
    "    def __init__(self, x, y, map, standard):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.m = map\n",
    "        self.s = standard\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.x[item], self.y[item], self.m[item], self.s[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf677fe3-3606-4e9e-a52c-9516da05f5e1",
   "metadata": {},
   "source": [
    "### RNN系列模型训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b984a353-1a50-4b83-8e76-398b21e1dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RNN(type, epoch=100, lr=0.01, batch_size=32, xtc_path='./CMA_dataset/xtc.npy',\n",
    "               ytc_path='./CMA_dataset/ytc.npy', split_index_path='./CMA_dataset/split_index.npy',\n",
    "               model_save_path='./other_models/checkpoints/'):\n",
    "    if type == 'LSTM':\n",
    "        model = LSTM_Model()\n",
    "        model_save_path = model_save_path + 'LSTM_demo.pth'\n",
    "    elif type == 'BiLSTM':\n",
    "        model = BiLSTM_Model()\n",
    "        model_save_path = model_save_path + 'BiLSTM_demo.pth'\n",
    "    elif type == 'GRU':\n",
    "        model = GRU_Model()\n",
    "        model_save_path = model_save_path + 'GRU_demo.pth'\n",
    "    elif type == 'BiGRU':\n",
    "        model = BiGRU_Model()\n",
    "        model_save_path = model_save_path + 'BiGRU_demo.pth'\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    xtc = np.load(xtc_path, allow_pickle=True).astype(float)\n",
    "    ytc = np.load(ytc_path, allow_pickle=True).astype(float)\n",
    "    standard = xtc[:, -1, :2].reshape(-1, 1, 2)\n",
    "    xtc, _ = data_scalar(xtc)\n",
    "\n",
    "    split_index = np.load(split_index_path, allow_pickle=True)\n",
    "    train_index = [*range(split_index[0], split_index[2])]\n",
    "    test_index = [*range(split_index[2], split_index[3])]\n",
    "    train_index, val_index, _, _ = train_test_split(train_index, train_index, test_size=0.2)\n",
    "\n",
    "    rnn_train_dataset = DataLoader(RNN_TrainLoader(xtc[train_index, :, :2], ytc[train_index], standard[train_index]),\n",
    "                                   batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    rnn_val_dataset = DataLoader(RNN_TrainLoader(xtc[val_index, :, :2], ytc[val_index], standard[val_index]),\n",
    "                                 batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    rnn_test_dataset = DataLoader(RNN_TrainLoader(xtc[test_index, :, :2], ytc[test_index], standard[test_index]),\n",
    "                                  batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    min_val_loss = 99999\n",
    "    for i in range(epoch):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, y, _ in rnn_train_dataset:\n",
    "            if torch.cuda.is_available():\n",
    "                x = x.float().cuda()\n",
    "                y = y.float().cuda()\n",
    "            else:\n",
    "                x = x.float()\n",
    "                y = y.float()\n",
    "            p = model(x)\n",
    "            loss = criterion(p, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.detach().item()\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        for x, y, _ in rnn_val_dataset:\n",
    "            if torch.cuda.is_available():\n",
    "                x = x.float().cuda()\n",
    "                y = y.float().cuda()\n",
    "            else:\n",
    "                x = x.float()\n",
    "                y = y.float()\n",
    "            p = model(x)\n",
    "            loss = criterion(p, y)\n",
    "            val_loss += loss.detach().item()\n",
    "        if val_loss < min_val_loss:\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            min_val_loss = val_loss\n",
    "        print('[%d/%d] Train Loss: %.5f / Val Loss: %.5f' % (i + 1, epoch, train_loss, val_loss))\n",
    "\n",
    "    state_dice = torch.load(model_save_path)\n",
    "    model.load_state_dict(state_dice)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        P, T, S = [], [], []\n",
    "        for x, y, s in rnn_test_dataset:\n",
    "            if torch.cuda.is_available():\n",
    "                x = x.float().cuda()\n",
    "            else:\n",
    "                x = x.float()\n",
    "            s = s.float()\n",
    "            y = y.float()\n",
    "            p = model(x)\n",
    "            P.append(p.detach().cpu().numpy())\n",
    "            S.append(s.detach().cpu().numpy())\n",
    "            T.append(y.detach().cpu().numpy())\n",
    "    P = np.concatenate(P, axis=0) + np.concatenate(S, axis=0)\n",
    "    T = np.concatenate(T, axis=0) + np.concatenate(S, axis=0)\n",
    "\n",
    "    for i in range(4):\n",
    "        d = compute_distance_km(P[:, i, 0], P[:, i, 1], T[:, i, 0], T[:, i, 1])\n",
    "        d = d.mean()\n",
    "        print('%dh: %.5f' % ((i + 1) * 6, d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd4468a-7056-42d6-a7cb-48903190f1f5",
   "metadata": {},
   "source": [
    "### CNN模型训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8734e249-164b-472d-90af-b27b5c17cfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_CNN(epoch=100, lr=0.01, batch_size=32, xtc_path='./CMA_dataset/xtc.npy',\n",
    "               ytc_path='./CMA_dataset/ytc.npy', split_index_path='./CMA_dataset/split_index.npy',\n",
    "               map_path='./gph.npy', model_save_path='./other_models/checkpoints/'):\n",
    "    model = CNN_Model()\n",
    "    model_save_path = model_save_path + 'CNN_demo.pth'\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    xtc = np.load(xtc_path, allow_pickle=True).astype(float)\n",
    "    ytc = np.load(ytc_path, allow_pickle=True).astype(float)\n",
    "    standard = xtc[:, -1, :2].reshape(-1, 1, 2)\n",
    "    xtc, _ = data_scalar(xtc)\n",
    "    xmap = np.load(map_path, allow_pickle=True).astype(float)\n",
    "    xmap, _ = data_scalar(xmap)\n",
    "\n",
    "    split_index = np.load(split_index_path, allow_pickle=True)\n",
    "    train_index = [*range(split_index[0], split_index[2])]\n",
    "    test_index = [*range(split_index[2], split_index[3])]\n",
    "    train_index, val_index, _, _ = train_test_split(train_index, train_index, test_size=0.2)\n",
    "\n",
    "    cnn_train_dataset = DataLoader(\n",
    "        CNN_TrainLoader(xtc[train_index, 1:, :2], ytc[train_index], xmap[train_index, 4:], standard[train_index]),\n",
    "        batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    cnn_val_dataset = DataLoader(\n",
    "        CNN_TrainLoader(xtc[val_index, 1:, :2], ytc[val_index], xmap[val_index, 4:], standard[val_index]),\n",
    "        batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    cnn_test_dataset = DataLoader(\n",
    "        CNN_TrainLoader(xtc[test_index, 1:, :2], ytc[test_index], xmap[test_index, 4:], standard[test_index]),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "    del xtc, ytc, xmap\n",
    "    \n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    min_val_loss = 99999\n",
    "    for i in range(epoch):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, y, m, _ in cnn_train_dataset:\n",
    "            if torch.cuda.is_available():\n",
    "                x = x.float().cuda()\n",
    "                y = y.float().cuda()\n",
    "                m = m.float().cuda()\n",
    "            else:\n",
    "                x = x.float()\n",
    "                y = y.float()\n",
    "                m = m.float()\n",
    "            p = model(x, m)\n",
    "            loss = criterion(p, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.detach().item()\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        for x, y, m, _ in cnn_val_dataset:\n",
    "            if torch.cuda.is_available():\n",
    "                x = x.float().cuda()\n",
    "                y = y.float().cuda()\n",
    "                m = m.float().cuda()\n",
    "            else:\n",
    "                x = x.float()\n",
    "                y = y.float()\n",
    "                m = m.float()\n",
    "            p = model(x, m)\n",
    "            loss = criterion(p, y)\n",
    "            val_loss += loss.detach().item()\n",
    "        if val_loss < min_val_loss:\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            min_val_loss = val_loss\n",
    "        print('[%d/%d] Train Loss: %.5f / Val Loss: %.5f' % (i + 1, epoch, train_loss, val_loss))\n",
    "\n",
    "    state_dice = torch.load(model_save_path)\n",
    "    model.load_state_dict(state_dice)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        P, S, T = [], [], []\n",
    "        for x, y, m, s in cnn_test_dataset:\n",
    "            if torch.cuda.is_available():\n",
    "                x = x.float().cuda()\n",
    "                m = m.float().cuda()\n",
    "            else:\n",
    "                x = x.float()\n",
    "                m = m.float()\n",
    "            s = s.float()\n",
    "            y = y.float()\n",
    "            p = model(x, m)\n",
    "            P.append(p.detach().cpu().numpy())\n",
    "            S.append(s.detach().cpu().numpy())\n",
    "            T.append(y.detach().cpu().numpy())\n",
    "    P = np.concatenate(P, axis=0)\n",
    "    T = np.concatenate(T, axis=0)\n",
    "    print(np.concatenate([P, T], axis=2)[:20])\n",
    "    P = P + np.concatenate(S, axis=0)\n",
    "    T = T + np.concatenate(S, axis=0)\n",
    "    print(np.concatenate([P, T], axis=2)[:20])\n",
    "\n",
    "    for i in range(4):\n",
    "        d = compute_distance_km(P[:, i, 0], P[:, i, 1], T[:, i, 0], T[:, i, 1])\n",
    "        d = d.mean()\n",
    "        print('%dh: %.5f' % ((i + 1) * 6, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd22199d-3ecc-4e44-a382-b8e4ad12f57e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100] Train Loss: 788.64592 / Val Loss: 188.56736\n",
      "[2/100] Train Loss: 756.33360 / Val Loss: 184.05788\n",
      "[3/100] Train Loss: 743.71184 / Val Loss: 182.59880\n",
      "[4/100] Train Loss: 741.13760 / Val Loss: 182.59105\n",
      "[5/100] Train Loss: 739.12292 / Val Loss: 182.12083\n",
      "[6/100] Train Loss: 737.27419 / Val Loss: 183.26221\n",
      "[7/100] Train Loss: 735.97147 / Val Loss: 181.33246\n",
      "[8/100] Train Loss: 734.94627 / Val Loss: 181.63880\n",
      "[9/100] Train Loss: 734.39018 / Val Loss: 181.09516\n",
      "[10/100] Train Loss: 733.66704 / Val Loss: 181.27636\n",
      "[11/100] Train Loss: 732.08484 / Val Loss: 180.79805\n",
      "[12/100] Train Loss: 732.00015 / Val Loss: 180.70709\n",
      "[13/100] Train Loss: 732.03595 / Val Loss: 180.54508\n",
      "[14/100] Train Loss: 731.44854 / Val Loss: 180.34020\n",
      "[15/100] Train Loss: 730.86449 / Val Loss: 180.52571\n",
      "[16/100] Train Loss: 730.72565 / Val Loss: 180.51903\n",
      "[17/100] Train Loss: 729.95131 / Val Loss: 181.65755\n",
      "[18/100] Train Loss: 729.95904 / Val Loss: 180.74573\n",
      "[19/100] Train Loss: 730.55177 / Val Loss: 181.03675\n",
      "[20/100] Train Loss: 729.71373 / Val Loss: 180.12281\n",
      "[21/100] Train Loss: 729.49956 / Val Loss: 181.50095\n",
      "[22/100] Train Loss: 729.24238 / Val Loss: 179.91735\n",
      "[23/100] Train Loss: 728.24938 / Val Loss: 179.87145\n",
      "[24/100] Train Loss: 728.70166 / Val Loss: 179.72082\n",
      "[25/100] Train Loss: 727.92452 / Val Loss: 179.57351\n",
      "[26/100] Train Loss: 727.68939 / Val Loss: 179.92023\n",
      "[27/100] Train Loss: 726.75763 / Val Loss: 179.28789\n",
      "[28/100] Train Loss: 726.02094 / Val Loss: 179.59007\n",
      "[29/100] Train Loss: 725.70256 / Val Loss: 178.84532\n",
      "[30/100] Train Loss: 725.19613 / Val Loss: 178.69852\n",
      "[31/100] Train Loss: 725.08177 / Val Loss: 179.79369\n",
      "[32/100] Train Loss: 723.24896 / Val Loss: 177.88407\n",
      "[33/100] Train Loss: 721.72203 / Val Loss: 178.00612\n",
      "[34/100] Train Loss: 721.01354 / Val Loss: 178.40378\n",
      "[35/100] Train Loss: 719.89701 / Val Loss: 177.35359\n",
      "[36/100] Train Loss: 716.86213 / Val Loss: 176.87278\n",
      "[37/100] Train Loss: 716.22347 / Val Loss: 176.08060\n",
      "[38/100] Train Loss: 712.62298 / Val Loss: 175.45410\n",
      "[39/100] Train Loss: 710.88240 / Val Loss: 176.80605\n",
      "[40/100] Train Loss: 708.53715 / Val Loss: 174.85225\n",
      "[41/100] Train Loss: 707.44876 / Val Loss: 174.46823\n",
      "[42/100] Train Loss: 703.57804 / Val Loss: 175.75685\n",
      "[43/100] Train Loss: 703.11098 / Val Loss: 173.54262\n",
      "[44/100] Train Loss: 703.37279 / Val Loss: 174.47636\n",
      "[45/100] Train Loss: 701.66077 / Val Loss: 173.78752\n",
      "[46/100] Train Loss: 700.75245 / Val Loss: 173.52207\n",
      "[47/100] Train Loss: 699.66446 / Val Loss: 172.94332\n",
      "[48/100] Train Loss: 698.31368 / Val Loss: 172.35948\n",
      "[49/100] Train Loss: 695.02357 / Val Loss: 180.49446\n",
      "[50/100] Train Loss: 696.48445 / Val Loss: 172.36835\n",
      "[51/100] Train Loss: 695.22223 / Val Loss: 172.33602\n",
      "[52/100] Train Loss: 694.87922 / Val Loss: 175.87392\n",
      "[53/100] Train Loss: 693.21925 / Val Loss: 170.82773\n",
      "[54/100] Train Loss: 692.63269 / Val Loss: 170.58205\n",
      "[55/100] Train Loss: 694.05532 / Val Loss: 171.83124\n",
      "[56/100] Train Loss: 692.84187 / Val Loss: 171.28864\n",
      "[57/100] Train Loss: 691.60253 / Val Loss: 170.50934\n",
      "[58/100] Train Loss: 690.76929 / Val Loss: 170.85168\n",
      "[59/100] Train Loss: 692.60416 / Val Loss: 174.02126\n",
      "[60/100] Train Loss: 692.30396 / Val Loss: 172.81052\n",
      "[61/100] Train Loss: 691.29424 / Val Loss: 170.61689\n",
      "[62/100] Train Loss: 690.32818 / Val Loss: 169.62181\n",
      "[63/100] Train Loss: 688.97992 / Val Loss: 170.23705\n",
      "[64/100] Train Loss: 689.10007 / Val Loss: 172.99684\n",
      "[65/100] Train Loss: 688.73426 / Val Loss: 170.98199\n",
      "[66/100] Train Loss: 688.96160 / Val Loss: 168.45576\n",
      "[67/100] Train Loss: 687.00669 / Val Loss: 169.13904\n",
      "[68/100] Train Loss: 685.85632 / Val Loss: 170.18844\n",
      "[69/100] Train Loss: 686.94434 / Val Loss: 169.35061\n",
      "[70/100] Train Loss: 686.76314 / Val Loss: 171.05557\n",
      "[71/100] Train Loss: 684.06453 / Val Loss: 168.52311\n",
      "[72/100] Train Loss: 684.13703 / Val Loss: 168.56630\n",
      "[73/100] Train Loss: 682.26029 / Val Loss: 174.00161\n",
      "[74/100] Train Loss: 685.04274 / Val Loss: 172.39058\n",
      "[75/100] Train Loss: 681.94744 / Val Loss: 168.42436\n",
      "[76/100] Train Loss: 680.83222 / Val Loss: 168.75946\n",
      "[77/100] Train Loss: 681.92133 / Val Loss: 169.10380\n",
      "[78/100] Train Loss: 680.94485 / Val Loss: 167.50963\n",
      "[79/100] Train Loss: 680.54712 / Val Loss: 167.90660\n",
      "[80/100] Train Loss: 681.80765 / Val Loss: 167.87728\n",
      "[81/100] Train Loss: 684.79821 / Val Loss: 168.14757\n",
      "[82/100] Train Loss: 681.49295 / Val Loss: 168.22362\n",
      "[83/100] Train Loss: 681.25637 / Val Loss: 168.78288\n",
      "[84/100] Train Loss: 681.62050 / Val Loss: 168.79167\n",
      "[85/100] Train Loss: 679.99068 / Val Loss: 169.49278\n",
      "[86/100] Train Loss: 679.70987 / Val Loss: 167.33969\n",
      "[87/100] Train Loss: 681.04215 / Val Loss: 168.98659\n",
      "[88/100] Train Loss: 680.46222 / Val Loss: 168.71921\n",
      "[89/100] Train Loss: 679.98197 / Val Loss: 168.19967\n",
      "[90/100] Train Loss: 679.75942 / Val Loss: 168.00613\n",
      "[91/100] Train Loss: 679.75413 / Val Loss: 167.35641\n",
      "[92/100] Train Loss: 678.23811 / Val Loss: 169.71846\n",
      "[93/100] Train Loss: 681.49751 / Val Loss: 169.00636\n",
      "[94/100] Train Loss: 680.62520 / Val Loss: 167.52462\n",
      "[95/100] Train Loss: 679.28607 / Val Loss: 169.64464\n",
      "[96/100] Train Loss: 678.53692 / Val Loss: 167.06579\n",
      "[97/100] Train Loss: 679.92559 / Val Loss: 169.76386\n",
      "[98/100] Train Loss: 681.30136 / Val Loss: 167.79272\n",
      "[99/100] Train Loss: 680.07909 / Val Loss: 167.05727\n",
      "[100/100] Train Loss: 680.32678 / Val Loss: 167.87072\n",
      "6h: 75.98150\n",
      "12h: 176.05454\n",
      "18h: 288.30822\n",
      "24h: 407.51586\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_RNN(type='LSTM')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a445de0-e241-4c97-a23c-a2dffc2cd5d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100] Train Loss: 770.78941 / Val Loss: 182.61424\n",
      "[2/100] Train Loss: 739.75265 / Val Loss: 181.57361\n",
      "[3/100] Train Loss: 730.11693 / Val Loss: 180.69195\n",
      "[4/100] Train Loss: 729.70079 / Val Loss: 179.27196\n",
      "[5/100] Train Loss: 715.84140 / Val Loss: 176.00143\n",
      "[6/100] Train Loss: 707.05977 / Val Loss: 175.28598\n",
      "[7/100] Train Loss: 705.95195 / Val Loss: 175.31580\n",
      "[8/100] Train Loss: 704.10952 / Val Loss: 174.97755\n",
      "[9/100] Train Loss: 702.83133 / Val Loss: 175.87199\n",
      "[10/100] Train Loss: 702.08747 / Val Loss: 174.25660\n",
      "[11/100] Train Loss: 702.37961 / Val Loss: 175.61442\n",
      "[12/100] Train Loss: 698.38702 / Val Loss: 175.99973\n",
      "[13/100] Train Loss: 697.62822 / Val Loss: 175.39034\n",
      "[14/100] Train Loss: 697.30223 / Val Loss: 174.99462\n",
      "[15/100] Train Loss: 697.15369 / Val Loss: 174.39792\n",
      "[16/100] Train Loss: 696.94816 / Val Loss: 173.50287\n",
      "[17/100] Train Loss: 694.11364 / Val Loss: 175.14944\n",
      "[18/100] Train Loss: 693.90701 / Val Loss: 173.77728\n",
      "[19/100] Train Loss: 705.02688 / Val Loss: 172.24169\n",
      "[20/100] Train Loss: 693.01510 / Val Loss: 174.21863\n",
      "[21/100] Train Loss: 693.30969 / Val Loss: 175.43099\n",
      "[22/100] Train Loss: 694.43741 / Val Loss: 172.36561\n",
      "[23/100] Train Loss: 695.61511 / Val Loss: 175.28191\n",
      "[24/100] Train Loss: 707.02089 / Val Loss: 172.04670\n",
      "[25/100] Train Loss: 695.26334 / Val Loss: 172.88833\n",
      "[26/100] Train Loss: 699.20810 / Val Loss: 171.67684\n",
      "[27/100] Train Loss: 700.60755 / Val Loss: 172.81315\n",
      "[28/100] Train Loss: 712.16906 / Val Loss: 173.40411\n",
      "[29/100] Train Loss: 693.00420 / Val Loss: 172.17444\n",
      "[30/100] Train Loss: 692.55429 / Val Loss: 172.40871\n",
      "[31/100] Train Loss: 692.55974 / Val Loss: 172.02424\n",
      "[32/100] Train Loss: 694.92786 / Val Loss: 173.41259\n",
      "[33/100] Train Loss: 693.19888 / Val Loss: 175.88105\n",
      "[34/100] Train Loss: 690.54654 / Val Loss: 172.22045\n",
      "[35/100] Train Loss: 690.20689 / Val Loss: 170.69465\n",
      "[36/100] Train Loss: 695.13113 / Val Loss: 171.69836\n",
      "[37/100] Train Loss: 690.53607 / Val Loss: 172.38434\n",
      "[38/100] Train Loss: 692.00552 / Val Loss: 175.78752\n",
      "[39/100] Train Loss: 703.59790 / Val Loss: 171.85291\n",
      "[40/100] Train Loss: 688.80409 / Val Loss: 171.25787\n",
      "[41/100] Train Loss: 690.68944 / Val Loss: 171.58903\n",
      "[42/100] Train Loss: 689.50699 / Val Loss: 171.41404\n",
      "[43/100] Train Loss: 691.17687 / Val Loss: 173.31077\n",
      "[44/100] Train Loss: 687.45941 / Val Loss: 171.88011\n",
      "[45/100] Train Loss: 686.72030 / Val Loss: 170.54006\n",
      "[46/100] Train Loss: 686.40881 / Val Loss: 174.03470\n",
      "[47/100] Train Loss: 687.93771 / Val Loss: 170.97660\n",
      "[48/100] Train Loss: 684.99515 / Val Loss: 170.44387\n",
      "[49/100] Train Loss: 686.22259 / Val Loss: 170.00552\n",
      "[50/100] Train Loss: 685.78004 / Val Loss: 170.78613\n",
      "[51/100] Train Loss: 692.62215 / Val Loss: 169.71069\n",
      "[52/100] Train Loss: 686.28972 / Val Loss: 169.87804\n",
      "[53/100] Train Loss: 683.76414 / Val Loss: 170.35632\n",
      "[54/100] Train Loss: 686.79749 / Val Loss: 172.10874\n",
      "[55/100] Train Loss: 691.37458 / Val Loss: 172.52597\n",
      "[56/100] Train Loss: 684.59573 / Val Loss: 170.64066\n",
      "[57/100] Train Loss: 684.82333 / Val Loss: 178.33775\n",
      "[58/100] Train Loss: 693.50892 / Val Loss: 173.22815\n",
      "[59/100] Train Loss: 684.83859 / Val Loss: 171.59315\n",
      "[60/100] Train Loss: 683.11919 / Val Loss: 169.07083\n",
      "[61/100] Train Loss: 691.46552 / Val Loss: 170.61702\n",
      "[62/100] Train Loss: 686.95394 / Val Loss: 177.93030\n",
      "[63/100] Train Loss: 691.58015 / Val Loss: 170.50138\n",
      "[64/100] Train Loss: 686.16957 / Val Loss: 170.49783\n",
      "[65/100] Train Loss: 683.10095 / Val Loss: 170.83150\n",
      "[66/100] Train Loss: 682.02189 / Val Loss: 169.97202\n",
      "[67/100] Train Loss: 686.09241 / Val Loss: 171.25084\n",
      "[68/100] Train Loss: 682.26818 / Val Loss: 169.13569\n",
      "[69/100] Train Loss: 681.32905 / Val Loss: 170.02516\n",
      "[70/100] Train Loss: 685.44936 / Val Loss: 172.75988\n",
      "[71/100] Train Loss: 701.82979 / Val Loss: 170.70400\n",
      "[72/100] Train Loss: 680.82827 / Val Loss: 169.29748\n",
      "[73/100] Train Loss: 683.17520 / Val Loss: 169.55041\n",
      "[74/100] Train Loss: 679.53638 / Val Loss: 168.27009\n",
      "[75/100] Train Loss: 680.17566 / Val Loss: 172.29937\n",
      "[76/100] Train Loss: 697.62026 / Val Loss: 169.43036\n",
      "[77/100] Train Loss: 681.28139 / Val Loss: 169.77915\n",
      "[78/100] Train Loss: 678.86299 / Val Loss: 168.10144\n",
      "[79/100] Train Loss: 683.88007 / Val Loss: 168.44283\n",
      "[80/100] Train Loss: 681.54152 / Val Loss: 170.67308\n",
      "[81/100] Train Loss: 697.76794 / Val Loss: 177.26053\n",
      "[82/100] Train Loss: 700.66551 / Val Loss: 172.32333\n",
      "[83/100] Train Loss: 689.82023 / Val Loss: 169.39170\n",
      "[84/100] Train Loss: 688.02333 / Val Loss: 170.35524\n",
      "[85/100] Train Loss: 688.47271 / Val Loss: 173.03364\n",
      "[86/100] Train Loss: 683.06870 / Val Loss: 171.81186\n",
      "[87/100] Train Loss: 699.79830 / Val Loss: 170.31253\n",
      "[88/100] Train Loss: 681.33814 / Val Loss: 169.59760\n",
      "[89/100] Train Loss: 679.45228 / Val Loss: 169.97144\n",
      "[90/100] Train Loss: 683.22824 / Val Loss: 169.39363\n",
      "[91/100] Train Loss: 704.10074 / Val Loss: 172.65520\n",
      "[92/100] Train Loss: 695.85987 / Val Loss: 171.85442\n",
      "[93/100] Train Loss: 692.23680 / Val Loss: 171.07706\n",
      "[94/100] Train Loss: 690.88678 / Val Loss: 170.21604\n",
      "[95/100] Train Loss: 687.88800 / Val Loss: 173.93218\n",
      "[96/100] Train Loss: 686.95132 / Val Loss: 169.77418\n",
      "[97/100] Train Loss: 685.79456 / Val Loss: 169.16726\n",
      "[98/100] Train Loss: 682.73328 / Val Loss: 168.65115\n",
      "[99/100] Train Loss: 692.07868 / Val Loss: 169.36253\n",
      "[100/100] Train Loss: 680.63819 / Val Loss: 174.20798\n",
      "6h: 77.01060\n",
      "12h: 173.33193\n",
      "18h: 285.36413\n",
      "24h: 404.84791\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_RNN(type='BiLSTM')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "602a779a-e086-41ab-9e56-917e57505ded",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100] Train Loss: 786.24531 / Val Loss: 187.48085\n",
      "[2/100] Train Loss: 747.26189 / Val Loss: 179.49012\n",
      "[3/100] Train Loss: 752.25426 / Val Loss: 179.84745\n",
      "[4/100] Train Loss: 744.31030 / Val Loss: 183.58123\n",
      "[5/100] Train Loss: 748.29380 / Val Loss: 186.42392\n",
      "[6/100] Train Loss: 740.12223 / Val Loss: 179.19080\n",
      "[7/100] Train Loss: 759.20620 / Val Loss: 181.84983\n",
      "[8/100] Train Loss: 782.40994 / Val Loss: 193.12387\n",
      "[9/100] Train Loss: 750.33961 / Val Loss: 181.47858\n",
      "[10/100] Train Loss: 733.72164 / Val Loss: 179.63826\n",
      "[11/100] Train Loss: 733.25728 / Val Loss: 179.53201\n",
      "[12/100] Train Loss: 730.90103 / Val Loss: 179.28404\n",
      "[13/100] Train Loss: 738.33643 / Val Loss: 178.29329\n",
      "[14/100] Train Loss: 730.56379 / Val Loss: 178.49702\n",
      "[15/100] Train Loss: 728.83941 / Val Loss: 178.81122\n",
      "[16/100] Train Loss: 738.06842 / Val Loss: 178.44021\n",
      "[17/100] Train Loss: 729.47479 / Val Loss: 177.92941\n",
      "[18/100] Train Loss: 729.18426 / Val Loss: 178.56592\n",
      "[19/100] Train Loss: 728.04100 / Val Loss: 177.59852\n",
      "[20/100] Train Loss: 726.67167 / Val Loss: 177.59783\n",
      "[21/100] Train Loss: 728.11834 / Val Loss: 177.56776\n",
      "[22/100] Train Loss: 729.37549 / Val Loss: 180.87728\n",
      "[23/100] Train Loss: 746.98555 / Val Loss: 183.05606\n",
      "[24/100] Train Loss: 765.42914 / Val Loss: 187.59970\n",
      "[25/100] Train Loss: 771.54060 / Val Loss: 187.31606\n",
      "[26/100] Train Loss: 769.35857 / Val Loss: 186.73611\n",
      "[27/100] Train Loss: 766.08435 / Val Loss: 185.65453\n",
      "[28/100] Train Loss: 755.50652 / Val Loss: 180.37471\n",
      "[29/100] Train Loss: 759.04049 / Val Loss: 184.55073\n",
      "[30/100] Train Loss: 737.63246 / Val Loss: 178.74661\n",
      "[31/100] Train Loss: 727.37534 / Val Loss: 177.77597\n",
      "[32/100] Train Loss: 726.34568 / Val Loss: 177.66344\n",
      "[33/100] Train Loss: 726.17349 / Val Loss: 177.53541\n",
      "[34/100] Train Loss: 761.91685 / Val Loss: 184.76270\n",
      "[35/100] Train Loss: 732.32828 / Val Loss: 178.23763\n",
      "[36/100] Train Loss: 741.43615 / Val Loss: 178.57668\n",
      "[37/100] Train Loss: 726.85859 / Val Loss: 177.64615\n",
      "[38/100] Train Loss: 726.39440 / Val Loss: 178.74501\n",
      "[39/100] Train Loss: 747.60430 / Val Loss: 182.18609\n",
      "[40/100] Train Loss: 742.31236 / Val Loss: 178.24728\n",
      "[41/100] Train Loss: 727.36194 / Val Loss: 178.32605\n",
      "[42/100] Train Loss: 726.15032 / Val Loss: 177.12850\n",
      "[43/100] Train Loss: 730.09382 / Val Loss: 177.20783\n",
      "[44/100] Train Loss: 725.30389 / Val Loss: 176.43762\n",
      "[45/100] Train Loss: 736.61179 / Val Loss: 177.16835\n",
      "[46/100] Train Loss: 739.85188 / Val Loss: 180.75997\n",
      "[47/100] Train Loss: 749.19864 / Val Loss: 203.97354\n",
      "[48/100] Train Loss: 787.90048 / Val Loss: 185.58113\n",
      "[49/100] Train Loss: 826.86395 / Val Loss: 203.40850\n",
      "[50/100] Train Loss: 786.40253 / Val Loss: 183.07782\n",
      "[51/100] Train Loss: 732.62221 / Val Loss: 178.13814\n",
      "[52/100] Train Loss: 728.11944 / Val Loss: 178.02641\n",
      "[53/100] Train Loss: 726.76840 / Val Loss: 177.84925\n",
      "[54/100] Train Loss: 727.25488 / Val Loss: 177.64448\n",
      "[55/100] Train Loss: 728.70172 / Val Loss: 178.01218\n",
      "[56/100] Train Loss: 729.22579 / Val Loss: 177.47299\n",
      "[57/100] Train Loss: 721.09478 / Val Loss: 177.00595\n",
      "[58/100] Train Loss: 733.31171 / Val Loss: 177.32678\n",
      "[59/100] Train Loss: 723.40252 / Val Loss: 176.13296\n",
      "[60/100] Train Loss: 724.27842 / Val Loss: 176.85735\n",
      "[61/100] Train Loss: 725.21908 / Val Loss: 175.23068\n",
      "[62/100] Train Loss: 719.36140 / Val Loss: 175.89432\n",
      "[63/100] Train Loss: 718.78250 / Val Loss: 177.63095\n",
      "[64/100] Train Loss: 720.37624 / Val Loss: 175.95079\n",
      "[65/100] Train Loss: 722.75812 / Val Loss: 177.19788\n",
      "[66/100] Train Loss: 723.46842 / Val Loss: 176.04215\n",
      "[67/100] Train Loss: 722.45393 / Val Loss: 181.42787\n",
      "[68/100] Train Loss: 728.59120 / Val Loss: 176.32353\n",
      "[69/100] Train Loss: 725.70212 / Val Loss: 176.66721\n",
      "[70/100] Train Loss: 728.31006 / Val Loss: 180.40460\n",
      "[71/100] Train Loss: 732.91735 / Val Loss: 176.18700\n",
      "[72/100] Train Loss: 719.22667 / Val Loss: 175.18299\n",
      "[73/100] Train Loss: 716.99217 / Val Loss: 174.67537\n",
      "[74/100] Train Loss: 721.33563 / Val Loss: 176.79347\n",
      "[75/100] Train Loss: 716.30407 / Val Loss: 174.47269\n",
      "[76/100] Train Loss: 725.97697 / Val Loss: 178.87183\n",
      "[77/100] Train Loss: 726.02261 / Val Loss: 175.70356\n",
      "[78/100] Train Loss: 734.06891 / Val Loss: 175.88786\n",
      "[79/100] Train Loss: 719.68811 / Val Loss: 175.11909\n",
      "[80/100] Train Loss: 717.65915 / Val Loss: 175.07764\n",
      "[81/100] Train Loss: 719.51327 / Val Loss: 174.49087\n",
      "[82/100] Train Loss: 722.08818 / Val Loss: 176.16517\n",
      "[83/100] Train Loss: 747.99765 / Val Loss: 179.26129\n",
      "[84/100] Train Loss: 727.57252 / Val Loss: 177.84392\n",
      "[85/100] Train Loss: 720.87547 / Val Loss: 175.59780\n",
      "[86/100] Train Loss: 719.21619 / Val Loss: 177.16931\n",
      "[87/100] Train Loss: 720.62808 / Val Loss: 176.21514\n",
      "[88/100] Train Loss: 720.53395 / Val Loss: 174.86208\n",
      "[89/100] Train Loss: 722.63044 / Val Loss: 176.98694\n",
      "[90/100] Train Loss: 719.22804 / Val Loss: 174.46645\n",
      "[91/100] Train Loss: 715.42388 / Val Loss: 175.39302\n",
      "[92/100] Train Loss: 715.77183 / Val Loss: 174.25923\n",
      "[93/100] Train Loss: 716.46894 / Val Loss: 174.70381\n",
      "[94/100] Train Loss: 714.61702 / Val Loss: 173.95364\n",
      "[95/100] Train Loss: 713.03136 / Val Loss: 176.29395\n",
      "[96/100] Train Loss: 716.27357 / Val Loss: 177.56617\n",
      "[97/100] Train Loss: 717.14123 / Val Loss: 175.12431\n",
      "[98/100] Train Loss: 711.28618 / Val Loss: 177.71000\n",
      "[99/100] Train Loss: 711.93522 / Val Loss: 174.28941\n",
      "[100/100] Train Loss: 711.97808 / Val Loss: 173.77647\n",
      "6h: 87.71161\n",
      "12h: 182.63957\n",
      "18h: 295.61284\n",
      "24h: 414.32295\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_RNN(type='GRU')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9850b477-09eb-4a6a-bab8-acc40de2fc81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100] Train Loss: 751.74101 / Val Loss: 180.75455\n",
      "[2/100] Train Loss: 732.12414 / Val Loss: 179.71873\n",
      "[3/100] Train Loss: 727.02035 / Val Loss: 178.04756\n",
      "[4/100] Train Loss: 718.80516 / Val Loss: 175.85080\n",
      "[5/100] Train Loss: 715.33475 / Val Loss: 175.34014\n",
      "[6/100] Train Loss: 708.07768 / Val Loss: 176.45782\n",
      "[7/100] Train Loss: 703.36910 / Val Loss: 174.68357\n",
      "[8/100] Train Loss: 704.61016 / Val Loss: 172.36951\n",
      "[9/100] Train Loss: 702.48134 / Val Loss: 172.06246\n",
      "[10/100] Train Loss: 700.65669 / Val Loss: 173.50328\n",
      "[11/100] Train Loss: 703.97978 / Val Loss: 173.89260\n",
      "[12/100] Train Loss: 704.50710 / Val Loss: 171.86448\n",
      "[13/100] Train Loss: 705.02984 / Val Loss: 172.19056\n",
      "[14/100] Train Loss: 702.48981 / Val Loss: 172.05721\n",
      "[15/100] Train Loss: 705.30167 / Val Loss: 175.85554\n",
      "[16/100] Train Loss: 708.71713 / Val Loss: 171.87095\n",
      "[17/100] Train Loss: 704.43779 / Val Loss: 172.10667\n",
      "[18/100] Train Loss: 703.84274 / Val Loss: 171.62364\n",
      "[19/100] Train Loss: 708.47512 / Val Loss: 184.82715\n",
      "[20/100] Train Loss: 702.60058 / Val Loss: 171.38556\n",
      "[21/100] Train Loss: 706.61312 / Val Loss: 179.57360\n",
      "[22/100] Train Loss: 711.93466 / Val Loss: 175.67699\n",
      "[23/100] Train Loss: 717.74061 / Val Loss: 175.84041\n",
      "[24/100] Train Loss: 700.37380 / Val Loss: 172.76999\n",
      "[25/100] Train Loss: 716.42947 / Val Loss: 178.62419\n",
      "[26/100] Train Loss: 723.57140 / Val Loss: 180.84137\n",
      "[27/100] Train Loss: 720.40770 / Val Loss: 174.74877\n",
      "[28/100] Train Loss: 702.81710 / Val Loss: 188.56464\n",
      "[29/100] Train Loss: 739.56521 / Val Loss: 180.47633\n",
      "[30/100] Train Loss: 713.74955 / Val Loss: 180.27125\n",
      "[31/100] Train Loss: 725.54627 / Val Loss: 175.27858\n",
      "[32/100] Train Loss: 718.05810 / Val Loss: 176.03670\n",
      "[33/100] Train Loss: 709.75268 / Val Loss: 175.38449\n",
      "[34/100] Train Loss: 708.10317 / Val Loss: 173.26661\n",
      "[35/100] Train Loss: 722.24600 / Val Loss: 181.07205\n",
      "[36/100] Train Loss: 730.85093 / Val Loss: 174.35409\n",
      "[37/100] Train Loss: 720.34541 / Val Loss: 172.41429\n",
      "[38/100] Train Loss: 700.85003 / Val Loss: 172.66103\n",
      "[39/100] Train Loss: 723.35301 / Val Loss: 191.61719\n",
      "[40/100] Train Loss: 707.32896 / Val Loss: 175.25179\n",
      "[41/100] Train Loss: 736.21717 / Val Loss: 185.64442\n",
      "[42/100] Train Loss: 748.61960 / Val Loss: 183.42042\n",
      "[43/100] Train Loss: 736.27072 / Val Loss: 179.93085\n",
      "[44/100] Train Loss: 728.08458 / Val Loss: 179.73359\n",
      "[45/100] Train Loss: 728.23952 / Val Loss: 179.06377\n",
      "[46/100] Train Loss: 725.95939 / Val Loss: 181.45295\n",
      "[47/100] Train Loss: 725.63888 / Val Loss: 177.86654\n",
      "[48/100] Train Loss: 718.90204 / Val Loss: 178.03417\n",
      "[49/100] Train Loss: 713.97932 / Val Loss: 179.43979\n",
      "[50/100] Train Loss: 710.00564 / Val Loss: 172.19451\n",
      "[51/100] Train Loss: 710.33376 / Val Loss: 172.71791\n",
      "[52/100] Train Loss: 715.96178 / Val Loss: 178.86329\n",
      "[53/100] Train Loss: 743.95854 / Val Loss: 182.32215\n",
      "[54/100] Train Loss: 728.81990 / Val Loss: 177.25971\n",
      "[55/100] Train Loss: 718.09085 / Val Loss: 177.75669\n",
      "[56/100] Train Loss: 714.26434 / Val Loss: 175.19419\n",
      "[57/100] Train Loss: 711.30445 / Val Loss: 177.72027\n",
      "[58/100] Train Loss: 712.77074 / Val Loss: 173.38635\n",
      "[59/100] Train Loss: 709.90962 / Val Loss: 172.63361\n",
      "[60/100] Train Loss: 748.48466 / Val Loss: 189.23736\n",
      "[61/100] Train Loss: 732.87211 / Val Loss: 177.57263\n",
      "[62/100] Train Loss: 721.00063 / Val Loss: 177.21668\n",
      "[63/100] Train Loss: 719.72858 / Val Loss: 175.38204\n",
      "[64/100] Train Loss: 722.78969 / Val Loss: 179.58475\n",
      "[65/100] Train Loss: 730.09963 / Val Loss: 183.82059\n",
      "[66/100] Train Loss: 719.21626 / Val Loss: 174.95324\n",
      "[67/100] Train Loss: 724.70114 / Val Loss: 177.19843\n",
      "[68/100] Train Loss: 721.44796 / Val Loss: 177.26423\n",
      "[69/100] Train Loss: 712.78546 / Val Loss: 175.52938\n",
      "[70/100] Train Loss: 715.47407 / Val Loss: 181.78465\n",
      "[71/100] Train Loss: 723.82925 / Val Loss: 177.97701\n",
      "[72/100] Train Loss: 726.65644 / Val Loss: 180.90993\n",
      "[73/100] Train Loss: 748.16564 / Val Loss: 184.23071\n",
      "[74/100] Train Loss: 741.88551 / Val Loss: 179.98210\n",
      "[75/100] Train Loss: 756.21839 / Val Loss: 186.14581\n",
      "[76/100] Train Loss: 755.09945 / Val Loss: 186.91791\n",
      "[77/100] Train Loss: 752.46006 / Val Loss: 185.65455\n",
      "[78/100] Train Loss: 742.43433 / Val Loss: 179.21241\n",
      "[79/100] Train Loss: 735.73251 / Val Loss: 180.74152\n",
      "[80/100] Train Loss: 726.50336 / Val Loss: 179.03342\n",
      "[81/100] Train Loss: 724.41745 / Val Loss: 178.43105\n",
      "[82/100] Train Loss: 721.76779 / Val Loss: 180.47819\n",
      "[83/100] Train Loss: 720.78481 / Val Loss: 177.51151\n",
      "[84/100] Train Loss: 721.51380 / Val Loss: 179.75493\n",
      "[85/100] Train Loss: 719.34935 / Val Loss: 177.74265\n",
      "[86/100] Train Loss: 720.69351 / Val Loss: 175.89553\n",
      "[87/100] Train Loss: 715.57083 / Val Loss: 176.48961\n",
      "[88/100] Train Loss: 713.01706 / Val Loss: 175.23333\n",
      "[89/100] Train Loss: 710.85285 / Val Loss: 174.78021\n",
      "[90/100] Train Loss: 708.47070 / Val Loss: 174.99004\n",
      "[91/100] Train Loss: 706.09565 / Val Loss: 175.08476\n",
      "[92/100] Train Loss: 703.98241 / Val Loss: 173.20183\n",
      "[93/100] Train Loss: 702.56186 / Val Loss: 174.56980\n",
      "[94/100] Train Loss: 703.12977 / Val Loss: 172.31122\n",
      "[95/100] Train Loss: 700.37382 / Val Loss: 172.54553\n",
      "[96/100] Train Loss: 703.15085 / Val Loss: 172.31839\n",
      "[97/100] Train Loss: 697.99752 / Val Loss: 171.12133\n",
      "[98/100] Train Loss: 698.64249 / Val Loss: 171.95022\n",
      "[99/100] Train Loss: 697.29511 / Val Loss: 172.33313\n",
      "[100/100] Train Loss: 699.42747 / Val Loss: 172.31243\n",
      "6h: 84.41691\n",
      "12h: 180.98935\n",
      "18h: 294.50042\n",
      "24h: 413.72502\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_RNN(type='BiGRU')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ff3d87-43e6-4048-b90b-04f45b4bd4d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
